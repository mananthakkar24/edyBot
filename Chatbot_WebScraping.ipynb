{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping():\n",
    "    qs = input()\n",
    "    global flag2\n",
    "    global loading\n",
    "    \n",
    "    URL = 'https://www.google.com/search?q=' + qs\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    links = soup.findAll(\"a\")\n",
    "    all_links = []\n",
    "    \n",
    "    for link in links:\n",
    "        link_href = link.get('href')\n",
    "        if \"url?q=\" in link_href and not \"webcache\" in link_href:\n",
    "            all_links.append((link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]))\n",
    "    \n",
    "    flag = False\n",
    "    for link in all_links:\n",
    "        if 'https://en.wikipedia.org/wiki/' in link:\n",
    "            wiki = link\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    div0 = soup.find_all('div',class_=\"kvKEAb\")\n",
    "    div1 = soup.find_all(\"div\",class_=\"Ap5OSd\")\n",
    "    div2 = soup.find_all(\"div\",class_=\"nGphre\")\n",
    "    div3 = soup.find_all(\"div\",class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "    \n",
    "    if len(div0) != 0:\n",
    "        answer = div0[0].text\n",
    "    elif len(div1) != 0:\n",
    "        answer = div1[0].text + \"\\n\" + div1[0].find_next_sibling(\"div\").text\n",
    "    elif len(div2) != 0:\n",
    "        answer = div2[0].find_next(\"span\").text + \"\\n\" + div2[0].find_next(\"div\",class_=\"kCrYT\").text\n",
    "    elif len(div3) != 0:\n",
    "        answer = div3[1].text\n",
    "    elif flag==True:\n",
    "        page2 = requests.get(wiki)\n",
    "        soup = BeautifulSoup(page2.text, 'html.parser')\n",
    "        title = soup.select(\"#firstHeading\")[0].text\n",
    "        \n",
    "        paragraphs = soup.select(\"p\")\n",
    "        for para in paragraphs:\n",
    "            if bool(para.text.strip()):\n",
    "                answer = title + \"\\n\" + para.text\n",
    "                break\n",
    "    else:\n",
    "        answer = \"Sorry. I could not find the desired results.\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def clean():\\n    ans = web_scraping()\\n    print(ans)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def clean():\n",
    "    ans = web_scraping()\n",
    "    print(ans)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flask\n",
      "Flask\n",
      "Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. It has no database abstraction layer, form validation, or any other components where pre-existing... Wikipedia\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    web_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
